!pip install selenium
!pip install beautifulsoup4


from selenium import webdriver
from selenium.webdriver.chrome.service import Service
ds = Service()
driver = webdriver.Chrome(service=ds)



driver.get(r"https://www.mediaexpert.pl/komputery-i-tablety/laptopy-i-ultrabooki/laptopy?page=1")


from selenium.webdriver.common.by import By
import time
time.sleep(3)

# Akceptujemy wszystkie ciasteczka
try:
    accept_cookies_button = driver.find_element(By.ID, "onetrust-accept-btn-handler")
    accept_cookies_button.click()
    print("Kliknięto przycisk akceptacji ciasteczek")
except:
    print("Przycisk akceptacji ciasteczek nie został znaleziony")



elements = driver.find_elements(By.XPATH, "//div[contains(@class, 'box')]/h2/a[contains(@class, 'is-animate ui-link')]")

# Iterujemy po znalezionych elementach i wypisujemy ich teksty
for element in elements:
    print("Pobrany tekst:", element.text)


# /html/body/main/div[2]/div[2]/div[2]/div[2]/div/div[2]/div[2]/div/div/div/div/div/div[2]/div[3]/div/div/div[3]


from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
import time
import pandas as pd

# Konfiguracja WebDrivera
ds = Service()
driver = webdriver.Chrome(service=ds)

# Lista do przechowywania danych o produktach
products_data = []

# Przechodzimy przez wszystkie strony
for page in range(1, 2):  # Możesz zmienić zakres stron
    url = f"https://www.mediaexpert.pl/komputery-i-tablety/laptopy-i-ultrabooki/laptopy?page={page}"
    driver.get(url)
    
    # Czekamy chwilę, aby strona się załadowała
    time.sleep(3)
    
    # Akceptujemy ciasteczka, jeśli przycisk jest obecny
    try:
        accept_cookies_button = driver.find_element(By.ID, "onetrust-accept-btn-handler")
        accept_cookies_button.click()
        print("Kliknięto przycisk akceptacji ciasteczek")
    except:
        print("Przycisk akceptacji ciasteczek nie został znaleziony")

    # Pobieramy elementy dla każdego produktu
    products = driver.find_elements(By.XPATH, "//span/div[contains(@class, 'offer-box')]")
    
    for product in products:
        try:
            # Pobieramy tytuł produktu
            title = product.find_element(By.XPATH, ".//h2/a[contains(@class, 'is-animate ui-link')]").text
        except:
            title = "Brak danych"
        
        try:
            # Pobieramy cenę produktu
            price = product.find_element(By.XPATH, ".//div[contains(@class, 'main-price')]").text
        except:
            price = "Brak ceny"
        
        # Pobieramy szczegóły techniczne z tabeli
        try:
            # Szukamy tabeli w produkcie
            table = product.find_element(By.XPATH, ".//table[contains(@class, 'list attributes')]")
            rows = table.find_elements(By.XPATH, "./tbody/tr")
            
            # Zbieramy dane z tabeli do słownika
            details = {}
            for row in rows:
                try:
                    column_name = row.find_element(By.XPATH, "./th/span").text.strip()
                    value = row.find_element(By.XPATH, "./td/span").text.strip()
                    details[column_name] = value
                except Exception as e:
                    print(f"Błąd w przetwarzaniu szczegółów: {e}")
        except:
            details = {}
        
        # Tworzymy pełny słownik z wszystkimi szczegółami
        product_data = {
            "Tytuł": title,
            "Cena": price,
        }
        product_data.update(details)  # Dodajemy szczegóły jako osobne kolumny
        
        # Dodajemy produkt do listy
        products_data.append(product_data)

    print(f"--- Zakończono stronę {page} ---")


# Zapisujemy dane do CSV
import pandas as pd

df = pd.DataFrame(products_data)
df.to_csv("produkty_mediaexpert_szczegoly.csv", index=False)
print("Dane zapisano do pliku produkty_mediaexpert_szczegoly.csv")




df





!pip install scrapy


import scrapy
from scrapy.crawler import CrawlerProcess
import json

class JsonWriterPipeline(object):
    def open_spider(self, spider):
        pass

    def close_spider(self, spider):
        pass

    def process_item(self, item, spider):
        pass
    

class MySpider(scrapy.Spider):
    name = 'mySpider'
    start_urls = []

    custom_settings = {
        'LOG_LEVEL': 'INFO',
    }
    
    def parse(self, response):

        
    
    def _parse_listig(self, listing, response):

        item = {
            'link': 'N/A',
            'title':  'N/A',
            'Localisation':  'N/A',
            'price':  'N/A',
            'room_count':  'N/A',
            'space_sm': 'N/A',
            'floor':  'N/A',
            'representative': 'N/A'
        }

         # Extract the link

        # Extract title

        # Extract localisation

        # Extract price and additional fee

        # Extract rooms, space, and floor information from details

        # Extract representative (e.g., "Oferta prywatna" or other info)

        return item


process = CrawlerProcess({...
})

process.crawl(MySpider)
process.start()     

