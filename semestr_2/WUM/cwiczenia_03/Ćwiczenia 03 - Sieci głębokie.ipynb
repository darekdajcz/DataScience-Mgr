{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ćwiczenia 03 - Sieci głębokie\n",
    "\n",
    "Trenowanie sieci o większej liczbie warstw wymaga stosowania odpowiednich technik aby zapobiec wystąpieniu problemu *zanikających/eksplodujących gradientów*:\n",
    "\n",
    "1. Dobór odpowiedniej funkcji aktywacji i strategii inicjalizacji wag:\n",
    "    - Jako funkcję aktywacji można stosować ReLU i jej warianty,\n",
    "    - Strategia inicjalizacji wag zależy od funkcji aktywacji. Dla ReLU odpowiednia będzie `he_normal`/`he_uniform`.\n",
    "2. Batch Normalization:\n",
    "    - Polega na normalizacji i skalowaniu wyjść warstwy.\n",
    "    - Warstwę `BatchNormalization` można dodać bezpośrednio po normalizowanej warstwie lub przed funkcją aktywacji.\n",
    "\n",
    "## Przykład "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.Input(shape=(32, 32, 3)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256,  kernel_initializer=\"he_normal\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadania\n",
    "Stwórz model głęboki do klasyfikacji zbioru [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "1. Stwórz sieć posiadającą 20 warstw ukrytych po 100 neuronów.\n",
    "2. Ustaw w warstwach sieci parametry `activation=\"sigmoid\"` i `kernel_initializer=\"random_normal\"`. Nie stosuj żadnych innych metod normalizacji. Zaobserwuj problem zanikacjących gradientów.\n",
    "3. Porównaj działanie sieci z zadania 2 z siecią, która stosuje `activation=\"relu\"` i `kernel_initializer=\"he_normal\"`.\n",
    "4. Zastosuj metodę batch normalization do każdej warstwy sieci. Stwórz wykres krzywych uczenia dla danych treningowych/walidacyjnych. Porównaj szybkość uczenia i jakość modeli.\n",
    "5. Porównaj krzywe uczenia i jakość modeli stworzonych przy pomocy różnych optymalizatorów (`SGD`, `Adam`, `Nadam`, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.1, random_state=1)\n",
    "\n",
    "X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
